{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be80210a",
   "metadata": {},
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<br>\n",
    "<div class=\"toc\">\n",
    "    <ul class=\"toc-item\">\n",
    "        <li>\n",
    "            <span>\n",
    "                <a href=\"#1-Подготовка-окружения\">\n",
    "                    <span class=\"toc-item-num\">1&nbsp;&nbsp;</span>\n",
    "                    Подготовка окружения\n",
    "                </a>\n",
    "            </span>\n",
    "        </li>\n",
    "        <li>\n",
    "            <span>\n",
    "                <a href=\"#2-Загрузка-данных\">\n",
    "                    <span class=\"toc-item-num\">2&nbsp;&nbsp;</span>\n",
    "                    Загрузка данных\n",
    "                </a>\n",
    "            </span>\n",
    "        </li>\n",
    "        <li>\n",
    "            <span>\n",
    "                <a href=\"#3-Дообучение-предобученной-GPT\">\n",
    "                    <span class=\"toc-item-num\">3&nbsp;&nbsp;</span>\n",
    "                    Дообучение предобученной GPT\n",
    "                </a>\n",
    "            </span>\n",
    "            <ul class=\"toc-item\">\n",
    "                <li>\n",
    "                    <span>\n",
    "                        <a href=\"#3.1-Обучение-модели\">\n",
    "                            <span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>\n",
    "                            Обучение модели\n",
    "                        </a>\n",
    "                    </span>\n",
    "                </li>\n",
    "                <li>\n",
    "                    <span>\n",
    "                        <a href=\"#3.2-Генерация-текста\">\n",
    "                            <span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>\n",
    "                            Генерация текста\n",
    "                        </a>\n",
    "                    </span>\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>\n",
    "            <span>\n",
    "                <a href=\"#4-Общий-вывод\">\n",
    "                    <span class=\"toc-item-num\">4&nbsp;&nbsp;</span>\n",
    "                    Общий вывод\n",
    "                </a>\n",
    "            </span>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9d4a79",
   "metadata": {},
   "source": [
    "# Генеративные текстовые нейросети | Дообучение GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef168b4",
   "metadata": {},
   "source": [
    "**Постановка задачи:** натренировать и сравнить качество нескольких генеративных текстовых моделей на одном из заданных текстовых датасетов.\n",
    "\n",
    "**Источник данных:** [Harry Potter and the Methods of Rationality](https://hpmor.ru/).\n",
    "\n",
    "**Характер данных:** текст книги \"Гарри Поттер и методы рационального мышления\".\n",
    "\n",
    "**Основные этапы:** исследовать следующие нейросетевые архитектуры:\n",
    "\n",
    "1. Simple RNN с посимвольной и пословной токенизацией.\n",
    "2. Однонаправленная однослойная и многослойная LSTM c посимвольной токенизацией и токенизацией по словам и [на основе BPE](https://keras.io/api/keras_nlp/tokenizers/byte_pair_tokenizer/).\n",
    "3. Двунаправленная LSTM.\n",
    "4. *(На хорошую оценку)* трансформерная архитектура (GPT) \"с нуля\" [пример](https://keras.io/examples/generative/text_generation_gpt/).\n",
    "5. *(На отличную оценку)* дообучение предобученной GPT-сети [пример](https://github.com/ZotovaElena/RuGPT3_finetuning)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83f3ad7",
   "metadata": {},
   "source": [
    "<div style=\"background-color: blue; height: 2px; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4366f5f1",
   "metadata": {},
   "source": [
    "# Реализации\n",
    "\n",
    "1. [RNN с посимвольной токенизацией](https://github.com/MAILabs-Edu-2023/magai_lab3_gennn-nlp_lab/blob/main/RNN_char.ipynb)\n",
    "2. [RNN с пословной токенизацией](https://github.com/MAILabs-Edu-2023/magai_lab3_gennn-nlp_lab/blob/main/RNN_word.ipynb)\n",
    "3. [Однонаправленная LSTM + BPE](https://github.com/MAILabs-Edu-2023/magai_lab3_gennn-nlp_lab/blob/main/unidirectional_LSTM_BPE.ipynb)\n",
    "4. [Двунаправленная LSTM](https://github.com/MAILabs-Edu-2023/magai_lab3_gennn-nlp_lab/blob/main/bidirectional_LSTM.ipynb)\n",
    "5. [Архитектура GPT](https://github.com/MAILabs-Edu-2023/magai_lab3_gennn-nlp_lab/blob/main/GPT_architecture.ipynb)\n",
    "6. Дообучение GPT (текущий файл)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f98730",
   "metadata": {},
   "source": [
    "<div style=\"background-color: blue; height: 2px; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec375e8c",
   "metadata": {},
   "source": [
    "## 1 Подготовка окружения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed451da1",
   "metadata": {},
   "source": [
    "Установка необходимых библиотек:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d4336fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "!pip install --upgrade transformers accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842e6567",
   "metadata": {},
   "source": [
    "Импорт библиотек:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13be500e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import keras_nlp\n",
    "import transformers\n",
    "\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling, \\\n",
    "                         GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, \\\n",
    "                         PreTrainedTokenizerFast, GPT2LMHeadModel, GPT2TokenizerFast\n",
    "\n",
    "from utils.useful_funcs import split_into_train_valid_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b5833a",
   "metadata": {},
   "source": [
    "<div style=\"background-color: blue; height: 2px; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3945d6",
   "metadata": {},
   "source": [
    "## 2 Загрузка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eac88ee",
   "metadata": {},
   "source": [
    "Проверка наличия папки для хранения наборов данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "043568a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir('data/') == False:\n",
    "    os.mkdir('data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecaab53",
   "metadata": {},
   "source": [
    "Задание пути к файлу с основным набором данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e3bc04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_file = 'data/hpmor.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ea3c42",
   "metadata": {},
   "source": [
    "Формирование/загрузка набора данных в зависимости от его наличия:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33125089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded from data/hpmor.txt\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(path_file, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    print('Uploaded from', path_file)\n",
    "    \n",
    "except:\n",
    "    text = get_data('https://hpmor.ru/')\n",
    "    \n",
    "    with open(path_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(text)\n",
    "    \n",
    "    print('Saved to', path_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63cf2fc",
   "metadata": {},
   "source": [
    "Выведение на экран начала текста:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2d65043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'гарри поттер и методы рационального мышления. элиезер юдковский (less wrong). петуния вышла замуж не за дурсля, а за университетского профессора, и гарри попал в гораздо более благоприятную среду. у него были частные учителя, дискуссии с отцом, а главное — книги, сотни и тысячи научных и фантастических книг. в 11 лет гарри знаком с квантовой механикой, когнитивной психологией, теорией вероятностей и другими вещами. но гарри не просто вундеркинд, у него есть загадочная тёмная сторона, которая явн'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44fa154",
   "metadata": {},
   "source": [
    "Выведение на экран общего числа слов в тексте:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6283d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего слов: 559855\n"
     ]
    }
   ],
   "source": [
    "print('Всего слов:', len(text.split(' ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1b88fc",
   "metadata": {},
   "source": [
    "<div style=\"background-color: blue; height: 2px; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2b8ccb",
   "metadata": {},
   "source": [
    "## 3 Дообучение предобученной GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a708b1",
   "metadata": {},
   "source": [
    "### 3.1 Обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75aadbfd",
   "metadata": {},
   "source": [
    "Задание констант:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1629e745",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt2'\n",
    "model_path = f'pretrained_{model_name}/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032fa6fb",
   "metadata": {},
   "source": [
    "Задание функции дообучения предобученной нейросети:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ea91ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_gpt2(file_path: str, \n",
    "                  n_epochs: int, \n",
    "                  batch_size: int, \n",
    "                  block_size: int,\n",
    "                  model_name: str = model_name,\n",
    "                  model_path: str = model_path) -> None:\n",
    "    \n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    \n",
    "    dataset = TextDataset(\n",
    "        tokenizer = tokenizer,\n",
    "        file_path = file_path,\n",
    "        block_size = block_size,\n",
    "    )\n",
    "    \n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, \n",
    "        mlm=False,\n",
    "    )\n",
    "    \n",
    "    tokenizer.save_pretrained(model_path)\n",
    "    model.save_pretrained(model_path)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "      output_dir=model_path,\n",
    "      overwrite_output_dir=False,\n",
    "      per_device_train_batch_size=batch_size,\n",
    "      num_train_epochs=n_epochs\n",
    "  )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "          model=model,\n",
    "          args=training_args,\n",
    "          data_collator=data_collator,\n",
    "          train_dataset=dataset\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9c6a28",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6b94c3",
   "metadata": {},
   "source": [
    "Дообучение предобученной нейросети:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdb82412",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python310\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "C:\\Python310\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18870' max='18870' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18870/18870 3:41:29, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.963500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.705900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.595500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.524600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.482300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.442200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.414800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.385000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.361200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.344900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.329200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.318800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.311000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.305200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.290200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.267300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.263000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.256800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>1.256100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.250100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.241100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.228900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>1.227300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.216700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>1.214800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>1.213400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>1.209600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>1.207200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>1.200100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>1.198400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>1.188300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>1.186500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>1.191500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>1.189500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>1.180500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>1.181100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>1.185700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "finetune_gpt2(path_file, n_epochs=5, batch_size=8, block_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b735f8c8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc26b8e",
   "metadata": {},
   "source": [
    "### 3.2 Генерация текста"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec914269",
   "metadata": {},
   "source": [
    "Задание функции генерации текста:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5713d190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(sample: str, \n",
    "                  max_length: int = 100,\n",
    "                  model_path: str = model_path) -> str:\n",
    "    \n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    ids = tokenizer.encode(sample, return_tensors='pt')\n",
    "    \n",
    "    tokens = model.generate(\n",
    "        ids,\n",
    "        do_sample=True,\n",
    "        max_length=max_length,\n",
    "        pad_token_id=model.config.eos_token_id,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "    )\n",
    "    \n",
    "    text = tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e096519",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa79795f",
   "metadata": {},
   "source": [
    "Генерация текста:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e62b54c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'гарри поттер, чтобы не гарри между на мира или у него в моём время освещало что-то слово. в секунду взрывый голос сосредоточился от быстро на золотой исчез хогва Tig�дан и за беголосов хиркал кладаннизма. если бы случае падма эксперимент точку обвинесился годы на пришли гаррри поковидеть в память светлы, который и стоит преобител может, где и он воды дварить лишь не года… славный существовал продолжат ты подолжен черного, забрав и вытащилил вся, где сложние бы славным'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text('гарри поттер', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02281a79",
   "metadata": {},
   "source": [
    ">Модель дообучилась на дополнительных текстовых данных, однако продемонстрировала не самый качественный результат. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcf6328",
   "metadata": {},
   "source": [
    "<div style=\"background-color: blue; height: 2px; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94405403",
   "metadata": {},
   "source": [
    "## 4 Общий вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e56ed2",
   "metadata": {},
   "source": [
    "Проведённый эксперимент можно назвать успешным, однако для получения более качественных результатов необходимо обучить модель на большем числе итераций."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12465d7",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size: 20px; padding: 15px 0;\">\n",
    "    <a href=\"#Содержание\" data-toc-modified-id=\"Содержание\" style=\"text-decoration: none; color: #296eaa; border: 2px dashed #296eaa; opacity: 0.8; border-radius: 3px; padding: 10px 80px;\">\n",
    "        В начало файла ↑\n",
    "    </a>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
